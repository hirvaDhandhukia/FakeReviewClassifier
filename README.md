Data Cleaning and Encoding Labels: 0:1 

Tokenization: Used bert-base-uncased tokenizer 

Splitting (Train-Val-Test): 80-10-10 

Model: Bert with Sequence Classification of 2 labels 

Training loop args with batch-size, learning rate, early stopping, and best model loading.
